{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3545ea34",
   "metadata": {},
   "source": [
    "## YouTube Video ‚Üí AI Study Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86ef69e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b7bbab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c3eaf503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_video_id(url):\n",
    "    \"\"\"Extracts video ID from different YouTube URL formats.\"\"\"\n",
    "    # We use Regex to hunt for the 11-character ID after 'v=' or 'youtu.be/'\n",
    "    match = re.search(r\"(?:v=|youtu\\.be/)([a-zA-Z0-9_-]{11})\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def get_transcript(video_id):\n",
    "    \"\"\"Fetch transcript using the NEW API format.\"\"\"\n",
    "    try:\n",
    "        api = YouTubeTranscriptApi()\n",
    "        # The .fetch method grabs the subtitle object list\n",
    "        transcript = api.fetch(video_id)\n",
    "        # We join the list into a single long string of text\n",
    "        return \" \".join([t.text for t in transcript])\n",
    "\n",
    "    except TranscriptsDisabled:\n",
    "        return \"Error: Transcripts are disabled for this video.\"\n",
    "    except NoTranscriptFound:\n",
    "        return \"Error: No transcript found for this video.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d242320d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [00:00<00:00, 493.98it/s, Materializing param=shared.weight]                                                       \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Check if we have a GPU (CUDA) available to speed things up\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "# Load the tokenizer (translates text to numbers)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Load the model (the neural network) and move it to the GPU/CPU\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5d432f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunk(text_chunk):\n",
    "    \"\"\"\n",
    "    Summarizes a text chunk with enhanced settings for longer, more informative summaries.\n",
    "    \"\"\"\n",
    "    # Prompt for clear, detailed summary\n",
    "    prompt = f\"Summarize the following text clearly and in detail:\\n{text_chunk}\"\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1024\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate summary with enhanced parameters\n",
    "    summary_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,        # Allow longer summaries\n",
    "        num_beams=6,               # Explore more paths for higher quality\n",
    "        length_penalty=1.2,        # Favor longer outputs\n",
    "        min_length=100,            # Ensure minimum content\n",
    "        no_repeat_ngram_size=3,    # Reduce repetition\n",
    "        early_stopping=True,\n",
    "        temperature=0.7,           # Slightly creative output\n",
    "        top_p=0.9                  # Nucleus sampling\n",
    "    )\n",
    "\n",
    "    # Decode summary back to text\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b5a676c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=1200):\n",
    "    sentences = text.split(\". \")\n",
    "    chunks, current_chunk = [], \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Check if adding the next sentence exceeds our limit\n",
    "        if len(current_chunk) + len(sentence) < chunk_size:\n",
    "            current_chunk += sentence + \". \"\n",
    "        else:\n",
    "            # If full, seal the chunk and start a new one\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \". \"\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0c55e4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé¨ Processing video: https://youtu.be/T-D1OfcDW1M?si=8mdhYF3WSHOtI3CQ\n",
      "üéß Fetching transcript...\n",
      "üî™ Chunking transcript...\n",
      "   -> 6 chunks created.\n",
      "üß† Generating AI notes...\n",
      "   Summarizing chunk 1/6...\n",
      "   Summarizing chunk 2/6...\n",
      "   Summarizing chunk 3/6...\n",
      "   Summarizing chunk 4/6...\n",
      "   Summarizing chunk 5/6...\n",
      "   Summarizing chunk 6/6...\n",
      "\n",
      "==================================================\n",
      "üìù AI GENERATED NOTES\n",
      "==================================================\n",
      "- Retrieval-Augmented Generation (RAG) is a framework to help large language models be more accurate and more up-to-date. This refers to large language modeling, or LLMs, that generate text in response to a user query, referred to as a prompt. These models can have some undesirable behavior. I want to tell you an anecdote to illustrate what I'm saying. So my kids, they recently asked me this question: \"In our solar system, what planet has the most moons?\" And I read an article and the article said that Jupiter and 88 moons. So that's really great that you're asking this question.\n",
      "- I don't know. I don‚Äôt have a source. I'm out of date. So we have two problems. One is no source. And the second problem is that I am out ofdate. And these, in fact, are often observed as problematic when interacting with Large Language Models (LARMs). They're LLM challenges. Now, what would have happened if I'd taken a beat and first gone and looked up the answer on a reputable source likeNASA. Well, then I would have been able to say, \"Ah, okay! So the answer is Saturn with 146 moons.\" And in reality, this keeps changing because scientists keep on discovering more and more moons.\n",
      "- A large language model would confidently say, OK, I have been trained and from what I know in my parameters during my training, the answer is Jupiter. But, you know, we don't know. The largest language model is very confident in what it answered. Now, what happens when you add this retrieval-augmented part here? What does that mean? Now, instead of just relying on what the LLM knows, we are adding a content store. This could be open like the internet. This can be closed like some collection of documents, collection of policies, whatever. The point, though, now, is that the LGM first goes and talks to the content store and says, ‚ÄúHey, can you retrieve for me information that is relevant to what the user's query was?‚Äù And now, with this retrievation-augmented answer, it's not Jupiter anymore, we know that it is Saturn.\n",
      "- Now in RAG, the generative model actually has an instruction that says, \"No, no, no.\" \"First, go and retrieve relevant content.\" \"Combine that with the user's question and only then generate the answer.\" \"Now you can give evidence for why your response was what it was. And in fact, now you can prove why your answer was what you was. So now hopefully you can see, how does RAG help the two LLM challenges that I had mentioned before? So first of all,\n",
      "- Let's take a closer look at what's going to happen to the large language model if it doesn't have evidence to back it up. Let' s talk about what'll happen if we don' t have enough evidence. Let 'em know if there's anything we can do about it. Let me know what you're thinking, and I'll do my utmost to get it right. Thanks for your help. I'd love to hear from you.\n",
      "- Thank you for learning more about RAG and like and subscribe to the channel. We are both working to improve the retriever to give the large language model the best quality data on which to ground its response, and also the generative part so that the LLM can give the richest, best response, finally to the user when it generates the answer. Thank you to learn more about ARG and Like and Subscribe to the Channel. Thank You for Learning More About RAG And Like And Subscribe To The Channel. Thanks for Learning\n"
     ]
    }
   ],
   "source": [
    "def generate_video_notes(video_url):\n",
    "    print(f\"\\nüé¨ Processing video: {video_url}\")\n",
    "\n",
    "    video_id = extract_video_id(video_url)\n",
    "    if not video_id:\n",
    "        print(\"Invalid YouTube URL.\")\n",
    "        return\n",
    "\n",
    "    print(\"üéß Fetching transcript...\")\n",
    "    transcript = get_transcript(video_id)\n",
    "\n",
    "    if transcript.startswith(\"Error\"):\n",
    "        print(transcript)\n",
    "        return\n",
    "\n",
    "    print(\"üî™ Chunking transcript...\")\n",
    "    chunks = chunk_text(transcript)\n",
    "    print(f\"   -> {len(chunks)} chunks created.\")\n",
    "\n",
    "    print(\"üß† Generating AI notes...\")\n",
    "    notes = []\n",
    "\n",
    "    # Loop through chunks and summarize each one\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"   Summarizing chunk {i+1}/{len(chunks)}...\")\n",
    "        summary = summarize_chunk(chunk)\n",
    "        notes.append(f\"- {summary}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìù AI GENERATED NOTES\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\n\".join(notes))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = input(\"Paste YouTube URL: \")\n",
    "    generate_video_notes(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0454e6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3c5748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
