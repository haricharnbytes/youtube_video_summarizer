{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3545ea34",
   "metadata": {},
   "source": [
    "## YouTube Video â†’ AI Study Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "86ef69e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5b7bbab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c3eaf503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_video_id(url):\n",
    "    \"\"\"Extracts video ID from different YouTube URL formats.\"\"\"\n",
    "    # We use Regex to hunt for the 11-character ID after 'v=' or 'youtu.be/'\n",
    "    match = re.search(r\"(?:v=|youtu\\.be/)([a-zA-Z0-9_-]{11})\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def get_transcript(video_id):\n",
    "    \"\"\"Fetch transcript using the NEW API format.\"\"\"\n",
    "    try:\n",
    "        api = YouTubeTranscriptApi()\n",
    "        # The .fetch method grabs the subtitle object list\n",
    "        transcript = api.fetch(video_id)\n",
    "        # We join the list into a single long string of text\n",
    "        return \" \".join([t.text for t in transcript])\n",
    "\n",
    "    except TranscriptsDisabled:\n",
    "        return \"Error: Transcripts are disabled for this video.\"\n",
    "    except NoTranscriptFound:\n",
    "        return \"Error: No transcript found for this video.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d242320d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 282/282 [00:00<00:00, 611.42it/s, Materializing param=shared.weight]                                                       \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Check if GPU (CUDA) available \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5d432f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunk(text_chunk):\n",
    "    \"\"\"\n",
    "    Summarizes a text chunk with enhanced settings for longer, more informative summaries.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert content summarizer. Summarize the following text clearly, accurately, and in detail. \n",
    "Your summary should be easy to understand, concise, and preserve all important points. \n",
    "Organize the summary logically and use complete sentences. Highlight key topics, main arguments, and any critical examples.\n",
    "\n",
    "{text_chunk}\"\"\"\n",
    "\n",
    "    # Tokenize \n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1024\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,        \n",
    "        num_beams=6,               \n",
    "        length_penalty=1.2,        \n",
    "        min_length=100,            \n",
    "        no_repeat_ngram_size=3,    \n",
    "        early_stopping=True,\n",
    "        temperature=0.7,           \n",
    "        top_p=0.9                  \n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b5a676c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_with_overlap(text, chunk_size=1200, overlap=200):\n",
    "    \"\"\"\n",
    "    Splits text into chunks with overlap.\n",
    "    - chunk_size: approx number of tokens per chunk\n",
    "    - overlap: approx number of tokens to overlap between chunks\n",
    "    \"\"\"\n",
    "    sentences = text.split(\". \")\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    token_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = sentence.split()  \n",
    "        if token_count + len(sentence_tokens) <= chunk_size:\n",
    "            current_chunk.extend(sentence_tokens)\n",
    "            token_count += len(sentence_tokens)\n",
    "        else:\n",
    "            \n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            \n",
    "            # Start new chunk with overlap\n",
    "            current_chunk = current_chunk[-overlap:] if overlap < len(current_chunk) else current_chunk\n",
    "            current_chunk.extend(sentence_tokens)\n",
    "            token_count = len(current_chunk)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0c55e4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing video: https://youtu.be/lUljxdkolK8?si=4IfjuZiDMQC6F-nW\n",
      "Fetching transcript...\n",
      "ðŸ”ª Chunking transcript...\n",
      "   -> 9 chunks created.\n",
      "Generating AI notes...\n",
      "   Summarizing chunk 1/9...\n",
      "   Summarizing chunk 2/9...\n",
      "   Summarizing chunk 3/9...\n",
      "   Summarizing chunk 4/9...\n",
      "   Summarizing chunk 5/9...\n",
      "   Summarizing chunk 6/9...\n",
      "   Summarizing chunk 7/9...\n",
      "   Summarizing chunk 8/9...\n",
      "   Summarizing chunk 9/9...\n",
      "\n",
      "==================================================\n",
      "AI GENERATED NOTES\n",
      "==================================================\n",
      "- Physicists are able to reversibly change the dynamics of atoms, molecules, and spheres. Here's a look at how physicians can do just the same thing â€” and generate new data in the process â€” using physics as a way of resolving chaos in the world of generative AI. Phylogenetic phylogeny is based on quantum mechanics, and it's one of the most important fields in the field of theoretical cosmodynamics. It's also one of physics' most important applications.\n",
      "- I'm going to make a two-part video focusing on diffusion models. So let's take a closer look and see what they're all about. First of all, we're going to go back to Johnathan Ho and Yang Song's work. We're also going back to Yang Song, who introduced a fundamentally different point of view in 2021, which we usually call score-based diffusion models based on energy-based models and score matching. So, let' s see what it's all about here.\n",
      "- Let's look at the Markov chain. Now let's take a closer look at DDPM Markov chains. Letâ€™s take an infinitesimal step. Let'll look at an ItÃ´ Euclidean differential equation. And letâ€™s think of Brownian processes as well. Here's a summary of what we're going to do next. Now, let'll go back and look at what we are going to look at next.\n",
      "- The Markov chain describing the noising process in the DDPM framework converges to the ItÃ´ equation we've just seen, which will also reveal the drift and diffusion coefficients. The first step is to define a new quantity, which we'll call beta-tilde. Now, let's introduce a small time interval, delta-t. Next, we reinterpret the index i in terms of continuous time. Now letâ€™s take the first-order Taylor expansion of the left-hand term, which is possible because we make delta-T tend to zero. Now we can rewrite x at time t+delta-t as approximately equal to this new expression. The second term on the right happens to be the exact expression of the random Brownian process we saw earlier â€” the one responsible for those tiny random jumps in particles. And just like that, we arrived at our ItÃ´ Euclide.\n",
      "- So now we've figured it out. Here's a closer look at this ItÃ´ equation. We'll start with f and g. We still keep the original drift from the forward process. Now let's take a close look at reversing this equation. Let's start by replacing f, g with the DDPM coefficients to make this a little simpler. So letâ€™s go back a step further. Hereâ€™s what we're going to do with this reverse SDE. Letâ€™s first replace f & g, and then subtract an additional term.\n",
      "- So let's say we have a Gaussian distribution â€” for the sake of illustrations, we'll take a simple mixture of Gauss. Now we already saw that the effect of this noising process on the distribution is to transform it slowly into Gausson centered on zero. Now what's the effect on the score function? Well, adding Gaussie noise to a distribution as sort of a smoothing effect, which also smooths the score functions. At the beginning, when the data is still clean, the scorefunction is sharp and complex, but as we add more noise, it gradually flattens out â€” until, at the final time T, it becomes similar to the score of the normal distribution.\n",
      "- So let's take a closer look at diffusion models. What we want to do is to approximate the score function at every time t. We define a loss that measures how far our networkâ€™s output is from this true score, and we take the expectation over random time-steps t and data samples x-zero. So letâ€™s write this conditional score explicitely: we end up with this simple expression which only requires knowing X-zer0 the ground-truth, and a Gaussian noise epsilon, so we can write the conditional scoring depending only on epeslon. Now let'll see how we do that.\n",
      "- We need to discretise a continuous-time SDE. Here's how we do it. Hereâ€™s a very simple algorithm to discretize a Continuous-Time Euclidean Equation (SDE). It's just square root of our time increment times a sample from a normal distribution. And that's pretty much it, now we have an algorithm to generate samples using our trained neural network, which acts just as the one in DDPM. Now, let's look at a few examples.\n",
      "- Score-Based Diffusion is based on continuous stochastic differential Euclidean differential equations (SDEs) to describe a smooth evolution over time. It's based largely on ELBOs to approximate the score function of the data distribution. Score-Diffusion relies heavily on Denoising Score Matching to learn the score functions. I'm not sure what the next video will cover just yet, but I'll see you then.\n"
     ]
    }
   ],
   "source": [
    "def generate_video_notes(video_url):\n",
    "    print(f\"\\nProcessing video: {video_url}\")\n",
    "\n",
    "    video_id = extract_video_id(video_url)\n",
    "    if not video_id:\n",
    "        print(\"Invalid YouTube URL.\")\n",
    "        return\n",
    "\n",
    "    print(\"Fetching transcript...\")\n",
    "    transcript = get_transcript(video_id)\n",
    "\n",
    "    if transcript.startswith(\"Error\"):\n",
    "        print(transcript)\n",
    "        return\n",
    "\n",
    "    print(\"ðŸ”ª Chunking transcript...\")\n",
    "    chunks = chunk_text(transcript)\n",
    "    print(f\"   -> {len(chunks)} chunks created.\")\n",
    "\n",
    "    print(\"Generating AI notes...\")\n",
    "    notes = []\n",
    "\n",
    "    # Loop through chunks and summarize each one\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"   Summarizing chunk {i+1}/{len(chunks)}...\")\n",
    "        summary = summarize_chunk(chunk)\n",
    "        notes.append(f\"- {summary}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"AI GENERATED NOTES\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\n\".join(notes))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = input(\"Paste YouTube URL: \")\n",
    "    generate_video_notes(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5057207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
